{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Build and train model-checkpoint.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzTdNxqrVQyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "# code for setting up google colab\n",
        "!rm -r /content/sample_data\n",
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle\n",
        "!kaggle datasets download -d nih-chest-xrays/data\n",
        "!unzip data.zip\n",
        "!rm data.zip\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm_JkHpxUNBo",
        "colab_type": "text"
      },
      "source": [
        "## Skeleton Code\n",
        "\n",
        "The code below provides a skeleton for the model building & training component of your project. You can add/remove/build on code however you see fit, this is meant as a starting point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7oU0j5QUNBq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43698234-778a-460d-e04e-93fdc6cb63f1"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "from glob import glob\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import chain\n",
        "from random import sample \n",
        "import scipy\n",
        "import sklearn.model_selection as skl\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "from skimage import io\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet import ResNet50 \n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, plot_precision_recall_curve, f1_score, confusion_matrix, accuracy_score\n",
        "##Import any other stats/DL/ML packages you may need here. E.g. Keras, scikit-learn, etc."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjQX-f47UNBw",
        "colab_type": "text"
      },
      "source": [
        "## Do some early processing of your metadata for easier model training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gz-LEH5GUNBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "9abbe72c-e6e4-4530-9fb3-228084c7bf29"
      },
      "source": [
        "## Below is some helper code to read all of your full image filepaths into a dataframe for easier manipulation\n",
        "## Load the NIH data to all_xray_df\n",
        "all_xray_df = pd.read_csv('/data/Data_Entry_2017.csv')\n",
        "all_image_paths = {os.path.basename(x): x for x in \n",
        "                   glob(os.path.join('/data','images*', '*', '*.png'))}\n",
        "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
        "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
        "all_xray_df.sample(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9e080b2b2206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Below is some helper code to read all of your full image filepaths into a dataframe for easier manipulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## Load the NIH data to all_xray_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_xray_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/Data_Entry_2017.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m all_image_paths = {os.path.basename(x): x for x in \n\u001b[1;32m      5\u001b[0m                    glob(os.path.join('/data','images*', '*', '*.png'))}\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /data/Data_Entry_2017.csv does not exist: '/data/Data_Entry_2017.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0It6WHiAUNB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Here you may want to create some extra columns in your table with binary indicators of certain diseases \n",
        "## rather than working directly with the 'Finding Labels' column\n",
        "\n",
        "# Todo\n",
        "all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
        "all_labels = [x for x in all_labels if len(x)>0]\n",
        "print('All Labels ({}): {}'.format(len(all_labels), all_labels))\n",
        "for c_label in all_labels:\n",
        "    if len(c_label)>1: # leave out empty labels\n",
        "        all_xray_df[c_label] = all_xray_df['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\n",
        "all_xray_df.sample(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH8QHGG2UNB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Here we can create a new column called 'pneumonia_class' that will allow us to look at \n",
        "## images with or without pneumonia for binary classification\n",
        "\n",
        "all_xray_df['pneumonia_class'] = all_xray_df['Pneumonia'].map(lambda label: \"withPneumonia\" if label else \"withoutPneumonia\")\n",
        "all_xray_df['img_path'] = all_xray_df['Image Index'].map(lambda idx: all_image_paths[idx])\n",
        "all_xray_df.sample(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kZOy5vIUNB9",
        "colab_type": "text"
      },
      "source": [
        "## Create your training and testing data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K8A326LUNB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_splits(vargs):\n",
        "    \n",
        "    ## Either build your own or use a built-in library to split your original dataframe into two sets \n",
        "    ## that can be used for training and testing your model\n",
        "    ## It's important to consider here how balanced or imbalanced you want each of those sets to be\n",
        "    ## for the presence of pneumonia\n",
        "    train_data, valid_data = skl.train_test_split(vargs, \n",
        "                                   test_size = 0.2, \n",
        "                                   stratify = vargs['Pneumonia'])\n",
        "    ## making equal proportions of Pneumonia in both sets!\n",
        "    # traing data\n",
        "    p_inds = train_data[train_data.Pneumonia==1].index.tolist()\n",
        "    np_inds = train_data[train_data.Pneumonia==0].index.tolist()\n",
        "    \n",
        "    np_sample = sample(np_inds,len(p_inds))\n",
        "    train_data = train_data.loc[p_inds + np_sample]\n",
        "    \n",
        "    # validation data\n",
        "    p_inds = valid_data[valid_data.Pneumonia==1].index.tolist()\n",
        "    np_inds = valid_data[valid_data.Pneumonia==0].index.tolist()\n",
        "\n",
        "    np_sample = sample(np_inds,4*len(p_inds))\n",
        "    valid_data = valid_data.loc[p_inds + np_sample]\n",
        "    return train_data, valid_data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJfqIU_GUNCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data = create_splits(all_xray_df)\n",
        "fig, m_axs = plt.subplots(5,4, figsize = (16, 16))\n",
        "m_axs = m_axs.flatten()\n",
        "imgs = train_data['Image Index']\n",
        "ind=0\n",
        "\n",
        "for img, ax in zip(imgs, m_axs):\n",
        "    img = io.imread(all_image_paths[img])\n",
        "    ax.imshow(img,cmap='gray')\n",
        "    ax.set_title(all_xray_df.iloc[ind]['Finding Labels'])\n",
        "    ind=ind+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ELpcAw-UNCH",
        "colab_type": "text"
      },
      "source": [
        "# Now we can begin our model-building & training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fvBTpUKUNCI",
        "colab_type": "text"
      },
      "source": [
        "#### First suggestion: perform some image augmentation on your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAW0_hL2UNCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "def my_image_augmentation(vargs):\n",
        "    ## recommendation here to implement a package like Keras' ImageDataGenerator\n",
        "    ## with some of the built-in augmentations \n",
        "    \n",
        "    ## keep an eye out for types of augmentation that are or are not appropriate for medical imaging data\n",
        "    ## Also keep in mind what sort of augmentation is or is not appropriate for testing vs validation data\n",
        "    \n",
        "    ## STAND-OUT SUGGESTION: implement some of your own custom augmentation that's *not*\n",
        "    ## built into something like a Keras package\n",
        "    \n",
        "    # Todo\n",
        "    train_idg = ImageDataGenerator(rescale=1. / 255.0,\n",
        "                              horizontal_flip = True, \n",
        "                              vertical_flip = False, \n",
        "                              height_shift_range= 0.1, \n",
        "                              width_shift_range=0.1, \n",
        "                              rotation_range=20, \n",
        "                              shear_range = 0.1,\n",
        "                              zoom_range=0.1)\n",
        "    \n",
        "    val_idg = ImageDataGenerator(rescale=1. / 255.0)\n",
        "    if vargs == 'train':\n",
        "        return train_idg\n",
        "    else:\n",
        "        return val_idg\n",
        "\n",
        "\n",
        "def make_train_gen(train_data):\n",
        "    \n",
        "    ## Create the actual generators using the output of my_image_augmentation for your training data\n",
        "    ## Suggestion here to use the flow_from_dataframe library, e.g.:\n",
        "    \n",
        "    #     train_gen = my_train_idg.flow_from_dataframe(dataframe=train_df, \n",
        "    #                                          directory=None, \n",
        "    #                                          x_col = ,\n",
        "    #                                          y_col = ,\n",
        "    #                                          class_mode = 'binary',\n",
        "    #                                          target_size = , \n",
        "    #                                          batch_size = \n",
        "    #                                          )\n",
        "    # Todo\n",
        "    IMG_SIZE = (224, 224)\n",
        "    train_idg = my_image_augmentation('train')\n",
        "    train_gen = train_idg.flow_from_dataframe(dataframe=train_data, \n",
        "                                         directory=None, \n",
        "                                         x_col = 'img_path',\n",
        "                                         y_col = 'pneumonia_class',\n",
        "                                         class_mode = 'binary',\n",
        "                                         target_size = IMG_SIZE, \n",
        "                                         batch_size = 32\n",
        "                                         )\n",
        "    return train_gen\n",
        "\n",
        "\n",
        "def make_val_gen(valid_data):\n",
        "    \n",
        "    #     val_gen = my_val_idg.flow_from_dataframe(dataframe = val_data, \n",
        "    #                                              directory=None, \n",
        "    #                                              x_col = ,\n",
        "    #                                              y_col = ',\n",
        "    #                                              class_mode = 'binary',\n",
        "    #                                              target_size = , \n",
        "    #                                              batch_size = ) \n",
        "\n",
        "    # Todo\n",
        "    val_idg = my_image_augmentation('valid')\n",
        "    val_gen = val_idg.flow_from_dataframe(dataframe=valid_data, \n",
        "                                         directory=None, \n",
        "                                         x_col = 'img_path',\n",
        "                                         y_col = 'pneumonia_class',\n",
        "                                         class_mode = 'binary',\n",
        "                                         target_size = IMG_SIZE, \n",
        "                                         batch_size = 100)\n",
        "    return val_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp9iiulPUNCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## May want to pull a single large batch of random validation data for testing after each epoch:\n",
        "val_gen = make_val_gen(valid_data)\n",
        "valX, valY = val_gen.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJPp_5voUNCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## May want to look at some examples of our augmented training data. \n",
        "## This is helpful for understanding the extent to which data is being manipulated prior to training, \n",
        "## and can be compared with how the raw data look prior to augmentation\n",
        "train_gen = make_train_gen(train_data)\n",
        "t_x, t_y = next(train_gen)\n",
        "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
        "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
        "    c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
        "    if c_y == 1: \n",
        "        c_ax.set_title('Pneumonia')\n",
        "    else:\n",
        "        c_ax.set_title('No Pneumonia')\n",
        "    c_ax.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5G4nEgZUNCV",
        "colab_type": "text"
      },
      "source": [
        "## Build your model: \n",
        "\n",
        "Recommendation here to use a pre-trained network downloaded from Keras for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDAUBci-UNCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_pretrained_model():\n",
        "    \n",
        "    # model = VGG16(include_top=True, weights='imagenet')\n",
        "    # transfer_layer = model.get_layer(lay_of_interest)\n",
        "    # vgg_model = Model(inputs = model.input, outputs = transfer_layer.output)\n",
        "    \n",
        "    # Todo\n",
        "    model = VGG16(include_top=True, weights='imagenet')\n",
        "    transfer_layer = model.get_layer('block5_pool')\n",
        "    vgg_model = Model(inputs = model.input, outputs = transfer_layer.output)\n",
        "    model.summary()\n",
        "    return vgg_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kxrTMAjUNCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_my_model():\n",
        "    my_model = Sequential()\n",
        "    # ....add your pre-trained model, and then whatever additional layers you think you might\n",
        "    # want for fine-tuning (Flatteen, Dense, Dropout, etc.)\n",
        "    \n",
        "    # if you want to compile your model within this function, consider which layers of your pre-trained model, \n",
        "    # you want to freeze before you compile \n",
        "    \n",
        "    # also make sure you set your optimizer, loss function, and metrics to monitor\n",
        "    \n",
        "    # Todo\n",
        "    vgg_model = load_pretrained_model()\n",
        "    for layer in vgg_model.layers[0:17]:\n",
        "        layer.trainable = False\n",
        "    for layer in vgg_model.layers:\n",
        "        print(layer.name, layer.trainable)\n",
        "    my_model.add(vgg_model)\n",
        "    my_model.add(Flatten())\n",
        "    my_model.add(Dense(1, activation='relu'))\n",
        "    '''\n",
        "    img_dims = 224\n",
        "    # Input layer\n",
        "    inputs = Input(shape=(img_dims, img_dims, 3))\n",
        "\n",
        "    # First conv block\n",
        "    x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Second conv block\n",
        "    x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Third conv block\n",
        "    x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Fourth conv block\n",
        "    x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(rate=0.2)(x)\n",
        "\n",
        "    # Fifth conv block\n",
        "    x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "    x = Dropout(rate=0.2)(x)\n",
        "\n",
        "    # FC layer\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=512, activation='relu')(x)\n",
        "    x = Dropout(rate=0.7)(x)\n",
        "    x = Dense(units=128, activation='relu')(x)\n",
        "    x = Dropout(rate=0.5)(x)\n",
        "    x = Dense(units=64, activation='relu')(x)\n",
        "    x = Dropout(rate=0.3)(x)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(units=1, activation='sigmoid')(x)\n",
        "\n",
        "    # Creating model and compiling\n",
        "    my_model = Model(inputs=inputs, outputs=output)\n",
        "    '''\n",
        "    return my_model\n",
        "\n",
        "\n",
        "my_model =  build_my_model()\n",
        "## STAND-OUT Suggestion: choose another output layer besides just the last classification layer of your modele\n",
        "## to output class activation maps to aid in clinical interpretation of your model's results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CJ-H0GUUNCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Below is some helper code that will allow you to add checkpoints to your model,\n",
        "## This will save the 'best' version of your model by comparing it to previous epochs of training\n",
        "\n",
        "## Note that you need to choose which metric to monitor for your model's 'best' performance if using this code. \n",
        "## The 'patience' parameter is set to 10, meaning that your model will train for ten epochs without seeing\n",
        "## improvement before quitting\n",
        "\n",
        "# Todo\n",
        "\n",
        "\n",
        "optimizer = Adam(lr=1e-6)\n",
        "loss = 'binary_crossentropy'\n",
        "metrics = ['binary_accuracy']\n",
        "\n",
        "weight_path=\"my_model.hdf5\"\n",
        "'''\n",
        "checkpoint = ModelCheckpoint(weight_path, \n",
        "                             monitor= 'val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             mode= 'max', \n",
        "                             save_weights_only = True)\n",
        "\n",
        "early = EarlyStopping(monitor= 'val_loss', \n",
        "                      mode= 'max', \n",
        "                      patience=5)\n",
        "\n",
        "callbacks_list = [checkpoint, early]\n",
        "'''\n",
        "my_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = ModelCheckpoint(filepath=weight_path, save_best_only=True, save_weights_only=True)\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQecuv1_UNCi",
        "colab_type": "text"
      },
      "source": [
        "### Start training! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWTOnL0tUNCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## train your model\n",
        "\n",
        "# Todo\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "'''\n",
        "my_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "history = my_model.fit_generator(train_gen, \n",
        "                          validation_data = (valX, valY), \n",
        "                          epochs = 10,\n",
        "                          callbacks = callbacks_list)\n",
        "'''\n",
        "history = my_model.fit_generator(\n",
        "           train_gen, steps_per_epoch=train_gen.samples // batch_size, \n",
        "           epochs=epochs, validation_data=(valX, valY), \n",
        "           validation_steps=val_gen.samples // batch_size, callbacks=[checkpoint, lr_reduce])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir6X8aZrUNCn",
        "colab_type": "text"
      },
      "source": [
        "##### After training for some time, look at the performance of your model by plotting some performance statistics:\n",
        "\n",
        "Note, these figures will come in handy for your FDA documentation later in the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUlJeU8dUNCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## After training, make some predictions to assess your model's overall performance\n",
        "## Note that detecting pneumonia is hard even for trained expert radiologists, \n",
        "## so there is no need to make the model perfect.\n",
        "'''\n",
        "from keras.models import model_from_json\n",
        "model_path = \"my_model.json\" #path to saved model\n",
        "weight_path = \"my_model.hdf5\"\n",
        "json_file = open(model_path, \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "my_model = model_from_json(loaded_model_json)\n",
        "my_model.load_weights(weight_path)\n",
        "'''\n",
        "my_model.load_weights(weight_path)\n",
        "pred_Y = my_model.predict(valX, batch_size = 100, verbose = True)\n",
        "# mapping in range of [0,1]\n",
        "max(pred_Y), min(pred_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loAbwFqKUNCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_auc(t_y, p_y):\n",
        "    \n",
        "    ## Hint: can use scikit-learn's built in functions here like roc_curve\n",
        "    \n",
        "    # Todo\n",
        "    fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
        "    fpr, tpr, thresholds = roc_curve(t_y, p_y)\n",
        "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % ('Pneumonia', auc(fpr, tpr)))\n",
        "    c_ax.legend()\n",
        "    c_ax.set_xlabel('False Positive Rate')\n",
        "    c_ax.set_ylabel('True Positive Rate')\n",
        "    return\n",
        "\n",
        "def plot_pr(t_y, p_y):\n",
        "    fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
        "    precision, recall, thresholds = precision_recall_curve(t_y, p_y)\n",
        "    c_ax.plot(precision, recall, label = '%s (AP Score:%0.2f)'  % ('Pneumonia', average_precision_score(t_y,p_y)))\n",
        "    c_ax.legend()\n",
        "    c_ax.set_xlabel('Recall')\n",
        "    c_ax.set_ylabel('Precision')\n",
        "\n",
        "def  calc_f1(prec,recall):\n",
        "\n",
        "    return 2*(prec*recall)/(prec+recall)\n",
        "\n",
        "#Also consider plotting the history of your model training:\n",
        "\n",
        "def plot_history(history):\n",
        "    N = len(history.history[\"loss\"])\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "    plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss/Accuracy\")\n",
        "    plt.legend(loc=\"lower left\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jog66U1KUNCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## plot figures\n",
        "\n",
        "# Todo\n",
        "plot_history(history)\n",
        "plot_auc(valY, pred_Y)\n",
        "plot_pr(valY, pred_Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlC6xStEUNC0",
        "colab_type": "text"
      },
      "source": [
        "Once you feel you are done training, you'll need to decide the proper classification threshold that optimizes your model's performance for a given metric (e.g. accuracy, F1, precision, etc.  You decide) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmuxKDhnUNC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Find the threshold that optimize your model's performance,\n",
        "## and use that threshold to make binary classification. Make sure you take all your metrics into consideration.\n",
        "\n",
        "# Todo\n",
        "precision, recall, thresholds = precision_recall_curve(valY, pred_Y)\n",
        "\n",
        "precision_value = 0.8\n",
        "idx1 = (np.abs(precision - precision_value)).argmin() \n",
        "print('Precision is: '+ str(precision[idx1]))\n",
        "print('Recall is: '+ str(recall[idx1]))\n",
        "print('Threshold is: '+ str(thresholds[idx1]))\n",
        "print('F1 Score is: ' + str(calc_f1(precision[idx1],recall[idx1])))\n",
        "thresh1 = thresholds[idx1]\n",
        "\n",
        "recall_value = 0.8\n",
        "idx2 = (np.abs(recall - recall_value)).argmin() \n",
        "print('Precision is: '+ str(precision[idx2]))\n",
        "print('Recall is: '+ str(recall[idx2]))\n",
        "print('Threshold is: '+ str(thresholds[idx2]))\n",
        "print('F1 Score is: ' + str(calc_f1(precision[idx2],recall[idx2])))\n",
        "thresh2 = thresholds[idx2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4HB2a_tUNC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = accuracy_score(valY, np.round(pred_Y))*100\n",
        "cm = confusion_matrix(valY, np.round(pred_Y), labels=[1,0])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print('CONFUSION MATRIX ------------------')\n",
        "print(cm)\n",
        "\n",
        "print('\\nTEST METRICS ----------------------')\n",
        "precision = tp/(tp+fp)*100\n",
        "recall = tp/(tp+fn)*100\n",
        "print('Accuracy: {}%'.format(acc))\n",
        "print('Precision: {}%'.format(precision))\n",
        "print('Recall: {}%'.format(recall))\n",
        "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
        "\n",
        "print('\\nTRAIN METRIC ----------------------')\n",
        "print('Train acc: {}'.format(np.round((history.history['accuracy'][-1])*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8CzmolcUNC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_idg = ImageDataGenerator(rescale=1. / 255.0)\n",
        "test_gen = test_idg.flow_from_dataframe(dataframe=valid_data, \n",
        "                                         directory=None, \n",
        "                                         x_col = 'img_path',\n",
        "                                         y_col = 'pneumonia_class',\n",
        "                                         class_mode = 'binary',\n",
        "                                         target_size = IMG_SIZE, \n",
        "                                         batch_size = 1430)\n",
        "testX, testY = test_gen.next()\n",
        "valid_data['algorithm_output'] = list(map(lambda prob: 1 if prob > thresh1 else 0, my_model.predict(testX, batch_size = 1430, verbose = True)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VspUNNIjUNDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpT7EyKWUNDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']:\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(valid_data[valid_data[i]==1].Pneumonia.values,valid_data[data[i]==1].algorithm_output.values,labels=[1,0]).ravel()\n",
        "    sens = tp/(tp+fn)\n",
        "    spec = tn/(tn+fp)\n",
        "\n",
        "    print(i)\n",
        "    print('Sensitivity: '+ str(sens))\n",
        "    print('Specificity: ' +str(spec))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7y-DdZkUNDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Let's look at some examples of predicted v. true with our best model: \n",
        "\n",
        "# Todo\n",
        "\n",
        "fig, m_axs = plt.subplots(10, 10, figsize = (16, 16))\n",
        "i = 0\n",
        "for (c_x, c_y, c_ax) in zip(valX[0:100], valY[0:100], m_axs.flatten()):\n",
        "    c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
        "    if c_y == 1: \n",
        "        if pred_Y[i] > thresholds[idx1]:\n",
        "            c_ax.set_title('1, 1')\n",
        "        else:\n",
        "            c_ax.set_title('1, 0')\n",
        "    else:\n",
        "        if pred_Y[i] > thresholds[idx1]: \n",
        "            c_ax.set_title('0, 1')\n",
        "        else:\n",
        "            c_ax.set_title('0, 0')\n",
        "    c_ax.axis('off')\n",
        "    i=i+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiZaNT8IUNDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Just save model architecture to a .json:\n",
        "\n",
        "model_json = my_model.to_json()\n",
        "with open(\"my_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}